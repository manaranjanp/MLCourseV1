{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Using Decision Trees\n",
    "\n",
    "**Author:** Manaranjan Pradhan</br>\n",
    "**Email ID:** manaranjan@gmail.com</br>\n",
    "**LinkedIn:** https://www.linkedin.com/in/manaranjanpradhan/</br>\n",
    "**Website:** www.manaranjanp.com\n",
    "\n",
    "\n",
    "## 1. HR - Attrition Analytics\n",
    "\n",
    "Human Resources are critical resources of any organiazation. Organizations spend huge amount of time and money to hire and nuture their employees. It is a huge loss for companies if employees leave, especially the key resources. So if HR can predict weather employees are at risk for leaving the company, it will allow them to identify the attrition risks and help understand and provie necessary support to retain those employees or do preventive hiring to minimize the impact to the orgranization.\n",
    "\n",
    "## 2. Data Set\n",
    "\n",
    "This dataset is taken from kaggle https://www.kaggle.com/datasets/jacksonchou/hr-data-for-analytics\n",
    "\n",
    "\n",
    "### 2.1 Dependent variable\n",
    "\n",
    "Left : 0 if employee did not leave , 1 if left company\n",
    "\n",
    "### 2.2 Independent variables\n",
    "\n",
    "- **satisfaction_level** : means how much employee satisfied (0 less satisfied , 1 most satisfied)\n",
    "- **last_evaluation** : means employees' evaluation for last month (0 bad , 1 Excellent)\n",
    "- **number_project** : number of projects the employee worked on\n",
    "- **average_montly_hours** : average months employee spends at work per month\n",
    "- **time_spend_company** : years the employee spent in a company\n",
    "- **Work_accident** : 0 if he did not have an accident , 1 if had at least one\n",
    "- **promotion_last_5years** : 0 if he did not have any promotion in last 5 years , 1 if had at least one\n",
    "- **dept** : department in which employee works\n",
    "- **salary** : High, medium or low bracket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "sn.set_palette(\"tab10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_df = pd.read_csv('HR_comma_sep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.scatterplot(data = hr_df.sample(20, random_state = 48),\n",
    "               x = 'satisfaction_level',\n",
    "               y = 'last_evaluation',\n",
    "               hue = 'left');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finding Decision Rules or Boundaries\n",
    "\n",
    "- Decision rules or boundaries in decision trees are determined based on the feature values and their corresponding thresholds to create partitions that maximize the separation of different classes.\n",
    "\n",
    "- Decision trees make splits in the data based on features that provide the most information gain or reduction in impurity.\n",
    " \n",
    "- Impurity estimation is a concept used in decision tree classification models to measure the homogeneity or impurity of a set.\n",
    "\n",
    "There are several impurity estimation metrics commonly used in decision tree algorithms, including:\n",
    "\n",
    "- Gini impurity: It measures the probability of misclassifying a randomly chosen element in the dataset if it were randomly labeled according to the class distribution at that node. A lower Gini impurity indicates a more homogeneous node.\n",
    "\n",
    "$$Gini(p) = 1 - \\sum_{i=1}^{c} p_i^2$$\n",
    "\n",
    "- Entropy: It calculates the level of disorder or uncertainty in a set of samples. Entropy is maximum when all class labels are equally probable and decreases as the distribution becomes more skewed. The goal is to minimize entropy at each node.\n",
    "\n",
    "$$Entropy(p) = -p \\log_2(p) - (1-p) \\log_2(1-p)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Total Gini Impurity and Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gini impurity ranges between 0 and 0.5, \n",
    "- 0.0 represents a perfectly pure node (all samples belong to the same class)\n",
    "- 0.5 indicates maximum impurity (samples are evenly distributed across different classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_total = 1 - pow(7/20, 2) - pow(13/20, 2)\n",
    "gini_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If decision rule is satisfaction_level < 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.scatterplot(data = hr_df.sample(20, random_state = 48),\n",
    "               x = 'satisfaction_level',\n",
    "               y = 'last_evaluation',\n",
    "               hue = 'left');\n",
    "plt.axvline(x = 0.2, color = 'r', label = 'cut off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_1 = 1 - pow(2/2, 2) - pow(0/2, 2)\n",
    "gini_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_2 = 1 - pow(5/18, 2) - pow(13/18, 2)\n",
    "gini_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Information gain\n",
    "\n",
    "- Information gain is a concept used in decision trees to measure the reduction in entropy or impurity achieved by splitting the data based on a specific attribute. \n",
    "\n",
    "- Information gain is calculated by comparing the entropy of the parent node (before the split) with the weighted average entropy of the child nodes (after the split).\n",
    "\n",
    "Let's computer impurity after the split is:\n",
    "\n",
    "$$Gini_{childnodes} = [Gini_{leftchild} * P(left_{child}) + Gini_{rightchild} * P(right_{child})]$$\n",
    "\n",
    "So, the reduction in impurity i.e. information gain is \n",
    "\n",
    "$$Information Gain = Gini_{parent} - Gini_{childnodes}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_childnodes = ((2/20)*gini_1 + (18/20)*gini_2)\n",
    "gini_childnodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain = gini_total - gini_childnodes\n",
    "round(information_gain, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If decision rule is satisfaction_level < 0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sn.scatterplot(data = hr_df.sample(20, random_state = 48),\n",
    "               x = 'satisfaction_level',\n",
    "               y = 'last_evaluation',\n",
    "               hue = 'left');\n",
    "plt.axvline(x = 0.45, color = 'r', label = 'cut off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_1 = 1 - pow(5/7, 2) - pow(2/7, 2)\n",
    "gini_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_2 = 1 - pow(2/13, 2) - pow(11/13, 2)\n",
    "gini_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain = gini_total - ((7/20)*gini_1 + (13/20)*gini_2)\n",
    "round(information_gain, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "\n",
    "- Decision rule is *satisfaction_level < 0.45* is better than *satisfaction_level < 0.2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex1: Participant Exercise\n",
    "\n",
    "1. Find information gains for the following decision rules:\n",
    "    - last_evaluation is 0.55\n",
    "    - last_evaluation is 0.85\n",
    "    \n",
    "2. Which of the above decision rule or boundary has maximum information gain?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_encoded_df = pd.get_dummies(hr_df, columns=['dept', 'salary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_encoded_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building a Decision Tree\n",
    "\n",
    "\n",
    "### 7.1. Setting X and y Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = list(hr_encoded_df.columns)\n",
    "X_features.remove('left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hr_encoded_df[X_features]\n",
    "y = hr_encoded_df.left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Split Dataset into train and test\n",
    "\n",
    "- Train: 80%\n",
    "- Test: 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, \\\n",
    "y_train, y_test = train_test_split( X,\n",
    "                                    y,\n",
    "                                    test_size = 0.2,\n",
    "                                    random_state = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Build Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=5, criterion='gini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit( X_train, y_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4. Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame({\"actual\": y_test,\n",
    "                     \"predicted\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df.sample(10, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5. Measuring Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.6. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_df.actual, y_df.predicted, labels=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot = ConfusionMatrixDisplay(cm, display_labels=['Left', 'No Left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.7. Finding accuracy metrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_df.actual, y_df.predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.8. ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = tree.predict_proba(X_test)\n",
    "y_df['pred_probs'] = pred_probs[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, RocCurveDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_df.actual, y_df.pred_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RocCurveDisplay.from_predictions(y_df.actual, y_df.pred_probs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualize the Decision Tree\n",
    "\n",
    "One of the benefits of decision tree is the rules can be visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (50, 12))\n",
    "plot_tree(tree,\n",
    "          feature_names = X_features,\n",
    "          class_names = ['Not Left', 'Left'],\n",
    "          filled = True,\n",
    "          fontsize = 10);\n",
    "plt.savefig('tree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display.Image(\"tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame({'feature': X_features,\n",
    "                              'importance': tree.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = importance_df.sort_values('importance', ascending = False)\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df['cummulative_imp'] = importance_df.importance.cumsum()\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex2: Participant Exercise\n",
    "\n",
    "1. Build decision trees for the following combination of hyper paramters and measure Roc AUC of the models.\n",
    "    - max_depth is 10 and criteria is gini\n",
    "    - max_depth is 5 and criteria is entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex3: Participant Exercise\n",
    "\n",
    "1. Build decision trees with the top 5 features based on feature importane found in step 9 and measure model performace using Roc AUC and Confusion Matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Benefits of Decision Trees\n",
    "\n",
    "Decision trees offer several benefits over other machine learning models, including:\n",
    "\n",
    "- Interpretable and Easy to Understand: Decision trees provide a transparent and intuitive representation of the decision-making process. The tree structure, along with the decision rules at each node, can be easily visualized and interpreted, making it easier to explain the model's predictions to stakeholders.\n",
    "\n",
    "- Handling Nonlinear Relationships: Decision trees can effectively capture nonlinear relationships between features and the target variable. They can learn complex decision boundaries without requiring explicit transformations or interactions between variables.\n",
    "\n",
    "- Feature Importance: Decision trees can provide insights into feature importance. By examining how frequently and at which points features are used for splitting, decision trees can rank the features based on their predictive power, enabling feature selection and dimensionality reduction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
